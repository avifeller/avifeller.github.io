<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Research – Avi Feller</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-12ecfb2ecc36a6195821167d1e398e1c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Avi Feller</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">about</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./research.html" aria-current="page"> 
<span class="menu-text">research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./teaching.html"> 
<span class="menu-text">teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Feller_CV_2025_Sept.pdf"> 
<span class="menu-text">cv</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Research</h2>
   
  <ul>
  <li><a href="#select-working-papers" id="toc-select-working-papers" class="nav-link active" data-scroll-target="#select-working-papers">Select Working Papers</a></li>
  <li><a href="#forthcoming" id="toc-forthcoming" class="nav-link" data-scroll-target="#forthcoming">Forthcoming</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">2025</a></li>
  <li><a href="#published-2024" id="toc-published-2024" class="nav-link" data-scroll-target="#published-2024">2024</a></li>
  <li><a href="#published-2023" id="toc-published-2023" class="nav-link" data-scroll-target="#published-2023">2023</a></li>
  <li><a href="#published-2022" id="toc-published-2022" class="nav-link" data-scroll-target="#published-2022">2022</a></li>
  <li><a href="#published-2021" id="toc-published-2021" class="nav-link" data-scroll-target="#published-2021">2021</a></li>
  <li><a href="#published-2020" id="toc-published-2020" class="nav-link" data-scroll-target="#published-2020">2020</a></li>
  <li><a href="#published-2019" id="toc-published-2019" class="nav-link" data-scroll-target="#published-2019">2019</a></li>
  <li><a href="#published-2018" id="toc-published-2018" class="nav-link" data-scroll-target="#published-2018">2018</a></li>
  <li><a href="#published-2017" id="toc-published-2017" class="nav-link" data-scroll-target="#published-2017">2017</a></li>
  <li><a href="#published-2016" id="toc-published-2016" class="nav-link" data-scroll-target="#published-2016">2016</a></li>
  <li><a href="#published-2015" id="toc-published-2015" class="nav-link" data-scroll-target="#published-2015">2015 and earlier</a></li>
  <li><a href="#comments" id="toc-comments" class="nav-link" data-scroll-target="#comments">Comments</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Research</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>[<a class="text-decoration-none" target="_blank" href="https://scholar.google.com/citations?hl=en&amp;user=Mz7heb4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate"><strong>Google Scholar</strong></a>] [<a class="text-decoration-none" target="_blank" href="https://arxiv.org/a/feller_a_1.html"><strong>arXiv</strong></a>]</p>
<h2 id="select-working-papers" class="anchored">Select Working Papers</h2>
<p><strong>Challenges in statistics: A Dozen challenges in causality and causal inference.</strong> Carlos Cinelli, Avi Feller, Guido Imbens, Edward Kennedy, Sara Magliacane, and José Zubizarreta. [<a href="#" data-bs-target="#causal_challenges_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2508.17099" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="causal_challenges_abstract" class="collapse">
<div class="card card-body">Causality and causal inference have emerged as core research areas at the interface of modern statistics and domains including biomedical sciences, social sciences, computer science, and beyond. The field's inherently interdisciplinary nature -- particularly the central role of incorporating domain knowledge -- creates a rich and varied set of statistical challenges. Much progress has been made, especially in the last three decades, but there remain many open questions. Our goal in this discussion is to outline research directions and open problems we view as particularly promising for future work. Throughout we emphasize that advancing causal research requires a wide range of contributions, from novel theory and methodological innovations to improved software tools and closer engagement with domain scientists and practitioners.
</div><br></div>
<p><strong>Regularizing extrapolation in causal inference.</strong> David Arbour, Harsh Parikh, Bijan Niknam, Elizabeth Stuart, Kara Rudolph, and Avi Feller. [<a href="#" data-bs-target="#extrapolation_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2509.17180" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="extrapolation_abstract" class="collapse"><div class="card card-body">
Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel "bias-bias-variance" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest.
</div><br></div>
<p><strong>The balancing act in causal inference.</strong> Eli Ben-Michael, Avi Feller, David A. Hirshberg, and José Zubizarreta. [<a href="#" data-bs-target="#balancing_act_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2110.14831" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="balancing_act_abstract" class="collapse">
<div class="card card-body">The idea of covariate balance is at the core of causal inference. Inverse propensity weights play a central role because they are the unique set of weights that balance the covariate distributions of different treatment groups. We discuss two broad approaches to estimating these weights: the more traditional one, which fits a propensity score model and then uses the reciprocal of the estimated propensity score to construct weights, and the balancing approach, which estimates the inverse propensity weights essentially by the method of moments, finding weights that achieve balance in the sample. We review ideas from the causal inference, sample surveys, and semiparametric estimation literatures, with particular attention to the role of balance as a sufficient condition for robust inference. We focus on the inverse propensity weighting and augmented inverse propensity weighting estimators for the average treatment effect given strong ignorability and consider generalizations for a broader class of problems including policy evaluation and the estimation of individualized treatment effects.
</div><br></div>
<p><strong>Distribution-free assessment of population overlap in observational studies.</strong> Lihua Lei, Alexander D’Amour, Peng Ding, Avi Feller, and Jasjeet Sekhon. [<a href="#" data-bs-target="#overlap_testing_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="https://lihualei71.github.io/ovalue.pdf" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="overlap_testing_abstract" class="collapse">
<div class="card card-body">Overlap in baseline covariates between treated and control groups, also known as positivity or common support, is a common assumption in observational causal inference. Assessing this
assumption is often ad hoc, however, and can give misleading results. For example, the common practice of examining the empirical distribution of estimated propensity scores is heavily dependent on model specification and has poor uncertainty quantification. In this paper, we propose a formal statistical framework for assessing the extrema of the population propensity score; e.g., the propensity score lies in [0.1, 0.9] almost surely. We develop a family of upper
confidence bounds, which we term O-values, for this quantity. We show these bounds are valid in finite samples so long as the observations are independent and identically distributed, without requiring any further modeling assumptions on the data generating process. Finally, we demonstrate this approach using benchmark observational studies, showing how to build our proposed method into the observational causal inference workflow.
</div><br></div>
<h2 id="forthcoming" class="anchored">Forthcoming</h2>
<p><strong>Augmented balancing weights as linear regression (with discussion).</strong> David Bruns-Smith, Oliver Dukes, Avi Feller, and Betsy Ogburn. <em>Journal of the Royal Statistical Society (Series B).</em> [<a href="#" data-bs-target="#balancing_equiv_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2304.14545" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="balancing_equiv_abstract" class="collapse">
<div class="card card-body">We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate a "double selection" property. Finally, we generalize these results to linear estimands via the Riesz representer. Our framework "opens the black box" on these increasingly popular estimators and provides important insights into estimation choices for augmented balancing weights.
</div><br></div>
<p><strong>Using multiple outcomes to improve the Synthetic Control Method.</strong> Liyang Sun, Eli Ben-Michael, and Avi Feller. <em>Review of Economics and Statistics.</em> [<a href="#" class="text-decoration-none" data-bs-target="#scm_multioutcomes_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://arxiv.org/abs/2311.16260" class="text-decoration-none"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="scm_multioutcomes_abstract" class="collapse">
<div class="card card-body">When there are multiple outcome series of interest, Synthetic Control analyses typically proceed by estimating separate weights for each outcome. In this paper, we instead propose estimating a common set of weights across outcomes, by balancing either a vector of all outcomes or an index or average of them. Under a low-rank factor model, we show that these approaches lead to lower bias bounds than separate weights, and that averaging leads to further gains when the number of outcomes grows. We illustrate this via simulation and in a re-analysis of the impact of the Flint water crisis on educational outcomes.</div><br></div>
<p><strong>Statistical methods to estimate the impact of gun policy on gun violence.</strong> Eli Ben-Michael, Mitchell Doucette, Avi Feller, Alexander McCourt, and Elizabeth Stuart. In <em>Gun Violence: Statistical Issues,</em> edited by C. Loeffler, L. Xue, and J. Rosenberger. [<a href="#" class="text-decoration-none" data-bs-target="#gun_policy_chapter" data-bs-toggle="collapse">abstract</a>; <a href="https://arxiv.org/abs/2404.11506" class="text-decoration-none"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="gun_policy_chapter" class="collapse">
<div class="card card-body">Gun violence is a critical public health and safety concern in the United States. There is considerable variability in policy proposals meant to curb gun violence, ranging from increasing gun availability to deter potential assailants (e.g., concealed carry laws or arming school teachers) to restricting access to firearms (e.g., universal background checks or banning assault weapons). Many studies use state-level variation in the enactment of these policies in order to quantify their effect on gun violence. In this paper, we discuss the policy trial emulation framework for evaluating the impact of these policies, and show how to apply this framework to estimating impacts via difference-in-differences and synthetic controls when there is staggered adoption of policies across jurisdictions, estimating the impacts of right-to-carry laws on violent crime as a case study.</div><br></div>
<h2 id="section" class="anchored">2025</h2>
<p><strong>Ridge boosting is both robust and efficient.</strong> David Bruns-Smith, Zhongming Xie, and Avi Feller. <em>NeurIPS (spotlight).</em> [<a href="#" data-bs-target="#ridge_boost_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="ridge_boost_abstract" class="collapse"><div class="card card-body">
Estimators in statistics and machine learning must typically trade off between
efficiency, having low variance for a fixed target, and distributional robustness,
such as multiaccuracy, or having low bias over a range of possible targets. In
this paper, we consider a simple estimator, ridge boosting: starting with any
initial predictor, perform a single boosting step with (kernel) ridge regression.
Surprisingly, we show that ridge boosting simultaneously achieves both efficiency
and distributional robustness: for target distribution shifts that lie within an RKHS
unit ball, this estimator maintains low bias across all such shifts and has variance
at the semiparametric efficiency bound for each target. In addition to bridging
otherwise distinct research areas, this result has immediate practical value. Since
ridge boosting uses only data from the source distribution, researchers can train
a single model to obtain both robust and efficient estimates for multiple target
estimands at the same time, eliminating the need to fit separate semiparametric
efficient estimators for each target. We assess this approach through simulations
and an application estimating the age profile of retirement income.
</div><br></div>
<p><strong>Leveraging semantic similarity for experimentation with AI-generated treatments.</strong> Lei Shi, David Arbour, Raghavendra Addanki, Ritwik Sinha, and Avi Feller. <em>NeurIPS.</em> [<a href="#" data-bs-target="#double_kernel_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="double_kernel_abstract" class="collapse"><div class="card card-body">
Large Language Models (LLMs) enable a new form of digital experimentation
where treatments combine human and model-generated content in increasingly
sophisticated ways. The main methodological challenge in this setting is repre-
senting these high-dimensional treatments without losing their semantic meaning
or rendering analysis intractable. Here we address this problem by focusing on
learning low-dimensional representations that capture the underlying structure
of such treatments. These representations enable downstream applications such
as guiding generative models to produce meaningful treatment variants and fa-
cilitating adaptive assignment in online experiments. We propose double kernel
representation learning, which models the causal effect through the inner product
of kernel-based representations of treatments and user covariates. We develop
an alternating-minimization algorithm that learns these representations efficiently
from data and provide convergence guarantees under a low-rank factor model.
As an application of this framework, we introduce an adaptive design strategy
for online experimentation and demonstrate the method’s effectiveness through
numerical experiments.
</div><br></div>
<p><strong>Handling missing responses under cluster dependence with applications to language model evaluation.</strong> Zhenghao Zeng, David Arbour, Avi Feller, Ishita Dasgupta, Atanu Sinha, and Edward Kennedy. <em>NeurIPS.</em> [<a href="#" data-bs-target="#cluster_dependence_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="cluster_dependence_abstract" class="collapse"><div class="card card-body">
Human annotations play a crucial role in evaluating the performance of GenAI
models. Two common challenges in practice, however, are missing annotations
(the response variable of interest) and cluster dependence among human-AI inter-
actions (e.g., questions asked by the same user may be highly correlated). Reliable
inference must address both issues to achieve unbiased estimation and appropri-
ately quantify uncertainty when estimating average scores from human annotations.
In this paper, we analyze the doubly robust estimator, a widely used method in
missing data analysis and causal inference, applied to this setting and establish
novel theoretical properties under cluster dependence. We further illustrate our
findings through simulations and a real-world conversation quality dataset. Our
theoretical and empirical results underscore the importance of incorporating cluster
dependence in missing response problems to perform valid statistical inference.
</div><br></div>
<p><strong>The Impossibility of fair LLMs.</strong> Jacy Reese Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, and Chenhao Tan. <em>Association for Computational Linguistics (ACL).</em> [<a href="#" data-bs-target="#fair_llms_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2406.03198" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="fair_llms_abstract" class="collapse"><div class="card card-body">
The rise of general-purpose artificial intelligence (AI) systems, particularly large language models (LLMs), has raised pressing moral questions about how to reduce bias and ensure fairness at scale. Researchers have documented a sort of "bias" in the significant correlations between demographics (e.g., race, gender) in LLM prompts and responses, but it remains unclear how LLM fairness could be evaluated with more rigorous definitions, such as group fairness or fair representations. We analyze a variety of technical fairness frameworks and find inherent challenges in each that make the development of a fair LLM intractable. We show that each framework either does not logically extend to the general-purpose AI context or is infeasible in practice, primarily due to the large amounts of unstructured training data and the many potential combinations of human populations, use cases, and sensitive attributes. These inherent challenges would persist for general-purpose AI, including LLMs, even if empirical challenges, such as limited participatory input and limited measurement methods, were overcome. Nonetheless, fairness will remain an important type of model evaluation, and there are still promising research directions, particularly the development of standards for the responsibility of LLM developers, context-specific evaluations, and methods of iterative, participatory, and AI-assisted evaluation that could scale fairness across the diverse contexts of modern human-AI interaction.
</div><br></div>
<p><strong>US abortion bans and fertility.</strong> Suzanne Bell, Alexander Franks, David Arbour, Selena Anjur-Dietrich, Elizabeth Stuart, Eli Ben-Michael, Avi Feller, and Alison Gemmill. <em>Journal of the American Medical Association,</em> 333 (15): 1324-1332. [<a href="#" class="text-decoration-none" data-bs-target="#dobbs_fertility_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://jamanetwork.com/journals/jama/fullarticle/2830297" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="dobbs_fertility_abstract" class="collapse"><div class="card card-body">
<p><i>Question</i>. Was adoption of complete or 6-week abortion bans associated with differential changes in fertility rates?</p>

<p><i>Findings.</i> There were an estimated 1.01 additional births above expectation per 1000 reproductive-aged females in states following the adoption of abortion bans (60.55 observed vs 59.54 expected; 1.7% increase), equivalent to 22,180 excess births. Estimated differences were largest among racially minoritized individuals, those without a college degree, Medicaid beneficiaries, unmarried individuals, younger individuals, and those in southern states.</p>

<p><i>Meaning.</i> Estimated differences in fertility associated with abortion bans were largest among people experiencing structural disadvantage and in states with among the worst maternal and child health outcomes.</p></div><br></div>
<p><strong>US abortion bans and infant mortality.</strong> Alison Gemmill, Alexander Franks, Selena Anjur-Dietrich, Amy Ozinsky, David Arbour, Elizabeth Stuart, Eli Ben-Michael, Avi Feller, and Suzanne Bell. <em>Journal of the American Medical Association,</em> 333(15): 1315-1323. [<a href="#" class="text-decoration-none" data-bs-target="#dobbs_mortality_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://jamanetwork.com/journals/jama/fullarticle/2830298" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="dobbs_mortality_abstract" class="collapse"><div class="card card-body">
<p><i>Question</i>. Was the adoption of complete or 6-week abortion bans associated with changes in infant mortality rates?</p>

<p><i>Findings.</i>  This analysis of US national vital statistics data from 2012 through 2023 found higher than expected infant mortality in states after adoption of abortion bans (observed vs expected, 6.26 vs 5.93 per 1000 live births; relative increase, 5.60%). Estimated increases were relatively larger among infants who were Black, had congenital anomalies, or were born in southern states.</p>

<p><i>Meaning.</i>  Abortion bans were associated with increases in infant mortality. These increases were larger for populations that already experienced higher than average rates of infant mortality.</p></div><br></div>
<p><strong>Evaluation and incident prevention in an enterprise AI assistant.</strong> Akash Maharaj, David Arbour, Daniel Lee, Uttaran Bhattacharya, Anup Rao, Austin Zane, Avi Feller, Kun Qian, and Yunyao Li. <em>Proceedings of the AAAI Conference on Artificial Intelligence,</em> 39(28), 28931-28937. [<a href="#" data-bs-target="#AI_incident_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://ojs.aaai.org/index.php/AAAI/article/view/35161" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="AI_incident_abstract" class="collapse"><div class="card card-body">
Enterprise AI Assistants are increasingly deployed in domains where accuracy is paramount, making each erroneous output a potentially significant incident. This paper presents a comprehensive framework for monitoring, benchmarking, and continuously improving such complex, multi-component systems under active development by multiple teams. Our approach encompasses three key elements: (1) a hierarchical ``severity'' framework for incident detection that identifies and categorizes errors while attributing component-specific error rates, facilitating targeted improvements; (2) a scalable and principled methodology for benchmark construction, evaluation, and deployment, designed to accommodate multiple development teams, mitigate overfitting risks, and assess the downstream impact of system modifications; and (3) a continual improvement strategy leveraging multidimensional evaluation, enabling the identification and implementation of diverse enhancement opportunities. By adopting this holistic framework, organizations can systematically enhance the reliability and performance of their AI Assistants, ensuring their efficacy in critical enterprise environments. We conclude by discussing how this multifaceted evaluation approach opens avenues for various classes of enhancements, paving the way for more robust and trustworthy AI systems.
</div><br></div>
<p><strong>Estimating racial disparities in emergency general surgery.</strong> Eli Ben-Michael, Avi Feller, Rachel Kelz, and Luke Keele. <em>Journal of the Royal Statistical Society (Series A).</em> 188(4): 1125-1148.[<a href="#" data-bs-target="#egs_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a class="text-decoration-none" target="_blank" href="https://arxiv.org/abs/2209.04321"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="egs_abstract" class="collapse">
<div class="card card-body">Research documents that Black patients experience worse general surgery outcomes than white patients in the United States. In this paper, we focus on an important but less-examined category: the surgical treatment of emergency general surgery (EGS) conditions, which refers to medical emergencies where the injury is "endogenous," such as a burst appendix. Our goal is to assess racial disparities for common outcomes after EGS treatment using an administrative database of hospital claims in New York, Florida, and Pennsylvania, and to understand the extent to which differences are attributable to patient-level risk factors versus hospital-level factors. To do so, we use a class of linear weighting estimators that re-weight white patients to have a similar distribution of baseline characteristics as Black patients. This framework nests many common approaches, including matching and linear regression, but offers important advantages over these methods in terms of controlling imbalance between groups, minimizing extrapolation, and reducing computation time. Applying this approach to the claims data, we find that disparities estimates that adjust for the admitting hospital are substantially smaller than estimates that adjust for patient baseline characteristics only, suggesting that hospital-specific factors are important drivers of racial disparities in EGS outcomes.
</div><br></div>
<p><strong>Addressing missing data due to COVID-19: Two early childhood case studies.</strong> Avi Feller, Maia Connors, Christina Weiland, John Easton, Stacy Ehrlich Loewe, John Francis, Sarah Kabourek, Diana Leyva, Anna Shapiro, and Gloria Yeomans-Maldonado. <em>Journal of Research on Educational Effectiveness.</em> 18(1): 226-245. [<a href="#" class="text-decoration-none" data-bs-target="#covid_missing_data_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://www.tandfonline.com/doi/full/10.1080/19345747.2024.2321438" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="covid_missing_data_abstract" class="collapse">
<div class="card card-body">One part of COVID-19’s staggering impact on education has been to suspend or fundamentally alter ongoing education research projects. This paper addresses how to analyze the simple but fundamental example of a multi-cohort study in which student assessment data for the final cohort are missing because schools were closed, learning was virtual, and/or assessments were canceled or inconsistently collected due to COVID-19. We argue that current best-practice recommendations for addressing missing data may fall short in such studies because the assumptions that underpin these recommendations are violated. We then provide a new, simple decision-making framework for empirical researchers facing this situation and provide two empirical examples of how to apply this framework drawn from early childhood studies, one a cluster randomized trial and the other a descriptive longitudinal study. Based on this framework and the assumptions required to address the missing data, we advise against the standard recommendation of adjusting for missing outcomes (e.g., via imputation or weighting). Instead, changing the target quantity by restricting to fully-observed cohorts or by pivoting to focusing on an alternative outcome may be more appropriate.</div><br></div>
<p><strong>Improving the estimation of site-specific effects and their distribution in multisite trials.</strong> JoonHo Lee, Jonathan Che, Sophia Rabe-Hesketh, Avi Feller, Luke Miratrix. <em>Journal of Educational and Behavioral Statistics,</em> 50(5): 731-764. [<a href="#" data-bs-target="#multisite_dist_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://doi.org/10.3102/10769986241254286" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2308.06913" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="multisite_dist_abstract" class="collapse">
<div class="card card-body">In multisite trials, researchers are often interested in several inferential goals: estimating treatment effects for each site, ranking these effects, and studying their distribution. This study seeks to identify optimal methods for estimating these targets. Through a comprehensive simulation study, we assess two strategies and their combined effects: semiparametric modeling of the prior distribution and alternative posterior summary methods tailored to minimize specific loss functions. Our findings highlight that the success of different estimation strategies depends largely on the amount of within-site and between-site information available from the data. We discuss how our results can guide balancing the trade-offs associated with shrinkage in limited data environments.</div><br></div>
<h2 id="published-2024" class="anchored">2024</h2>
<p><strong>Reducing student absenteeism at scale: Translating social norms and attention interventions.</strong> Todd Rogers and Avi Feller. In D. Soman (Ed.), <em>What works, what doesn’t (and when): Case studies in applied behavioral science</em> (pp.&nbsp;290–312). University of Toronto Press. [<a href="#" class="text-decoration-none" data-bs-target="#rogers_book_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://toddrogers.scholars.harvard.edu/sites/g/files/omnuum4616/files/2025-05/Soman_7373-1p%20ch%2016.final_.pdf" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="rogers_book_abstract" class="collapse">
<div class="card card-body"><strong>Our Goal:</strong> To reduce student absenteeism nationwide through a social enterprise that implements replicated absence-reduction research for school districts. <strong>The Intervention:</strong> Repeatedly-delivered, mail-based, accessibly-written information interventions with dynamic content that focuses parent attention on absenteeism, while correcting parent misbeliefs about their student's total absences and how it compares to that of their classmates. <strong>The Results:</strong> Increased attendance, at-scale, and fifty times more cost effective than next best intervention. <strong>Lesson Learned:</strong> Student absenteeism can be reduced cost-effectively, district-wide, in a continuously improving way, especially for the most vulnerable students.</div><br></div>
<p><strong>Towards representation learning for weighting problems in design-based causal inference.</strong> Oscar Clivio, Avi Feller, and Chris Holmes. <em>Uncertainty in Artificial Intelligence (UAI).</em> [<a href="#" class="text-decoration-none" data-bs-target="#clivio_uai_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://openreview.net/pdf?id=KNgBCZXJkY" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="clivio_uai_abstract" class="collapse">
<div class="card card-body">Reweighting a distribution to minimize a distance to a target distribution is a powerful and flexible strategy for estimating a wide range of causal effects, but can be challenging in practice because optimal weights typically depend on knowledge of the underlying data generating process. In this paper, we focus on design-based weights, which do not incorporate outcome information; prominent examples include prospective cohort studies, survey weighting, and the weighting portion of augmented weighting estimators. In such applications, we explore the central role of representation learning in finding desirable weights in practice. Unlike the common approach of assuming a well-specified representation, we highlight the error due to the choice of a representation and outline a general framework for finding suitable representations that minimize this error. Building on recent work that combines balancing weights and neural networks, we propose an end-to-end estimation procedure that learns a flexible representation, while retaining promising theoretical properties. We show that this approach is competitive in a range of common causal inference tasks.</div><br></div>
<p><strong>Continuous treatment effects with surrogate outcomes.</strong> Zhenghao Zeng, David Arbour, Avi Feller, Raghavendra Addanki, Ryan Rossi, Ritwik Sinha, and Edward Kennedy. <em>International Conference on Machine Learning (ICML).</em> [<a href="#" class="text-decoration-none" data-bs-target="#continuous_surrogates_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://openreview.net/forum?id=cZNuYKtoOZ" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/2402.00168" class="text-decoration-none"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="continuous_surrogates_abstract" class="collapse">
<div class="card card-body">In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish the asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.</div><br></div>
<p><strong>Temporal aggregation for the Synthetic Control Method.</strong> Liyang Sun, Eli Ben-Michael, and Avi Feller. <em>AEA Papers &amp; Proceedings</em> 114: 1–5. [<a href="#" class="text-decoration-none" data-bs-target="#scm_temporal_abstract" data-bs-toggle="collapse">abstract</a>; <a href="https://arxiv.org/abs/2401.12084" class="text-decoration-none"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="scm_temporal_abstract" class="collapse">
<div class="card card-body">The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit with panel data. Two challenges arise with higher frequency data (e.g., monthly versus yearly): (1) achieving excellent pre-treatment fit is typically more challenging; and (2) overfitting to noise is more likely. Aggregating data over time can mitigate these problems but can also destroy important signal. In this paper, we bound the bias for SCM with disaggregated and aggregated outcomes and give conditions under which aggregating tightens the bounds. We then propose finding weights that balance both disaggregated and aggregated series.</div><br></div>
<p><strong>Randomization tests for peer effects in group formation experiments.</strong> Guillaume Basse, Peng Ding, Avi Feller, and Panos Toulis. <em>Econometrica.</em> 92(2): 567–590. [<a href="#" data-bs-target="#peer_effects_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.econometricsociety.org/publications/econometrica/2024/03/01/Randomization-tests-for-peer-effects-in-group-formation-experiments" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1904.02308" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="peer_effects_abstract" class="collapse">
<div class="card card-body">Measuring the effect of peers on individuals' outcomes is a challenging problem, in part because individuals often select peers who are similar in both observable and unobservable ways. Group formation experiments avoid this problem by randomly assigning individuals to groups and observing their responses; for example, do first‐year students have better grades when they are randomly assigned roommates who have stronger academic backgrounds? In this paper, we propose randomization‐based permutation tests for group formation experiments, extending classical Fisher Randomization Tests to this setting. The proposed tests are justified by the randomization itself, require relatively few assumptions, and are exact in finite samples. This approach can also complement existing strategies, such as linear‐in‐means models, by using a regression coefficient as the test statistic. We apply the proposed tests to two recent group formation experiments.</div><br></div>
<h2 id="published-2023" class="anchored">2023</h2>
<p><strong>Interpretable sensitivity analysis for balancing weights.</strong> Dan Soriano, Eli Ben-Michael, Peter Bickel, Avi Feller, and Sam Pimentel. <em>Journal of the Royal Statistical Society (Series A).</em> 186(4): 707–721. [<a href="#" data-bs-target="#sensitivity_bw_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://academic.oup.com/jrsssa/article-abstract/186/4/707/7083941" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2102.13218" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="sensitivity_bw_abstract" class="collapse">
<div class="card card-body">Assessing sensitivity to unmeasured confounding is an important step in observational studies, which typically estimate effects under the assumption that all confounders are measured. In this paper, we develop a sensitivity analysis framework for balancing weights estimators, an increasingly popular approach that solves an optimization problem to obtain weights that directly minimizes covariate imbalance. In particular, we adapt a sensitivity analysis framework using the percentile bootstrap for a broad class of balancing weights estimators. We prove that the percentile bootstrap procedure can, with only minor modifications, yield valid confidence intervals for causal effects under restrictions on the level of unmeasured confounding. We also propose an amplification—a mapping from a one-dimensional sensitivity analysis to a higher dimensional sensitivity analysis—to allow for interpretable sensitivity parameters in the balancing weights framework. We illustrate our method through extensive real data examples.
</div><br></div>
<p><strong>Variation in impacts of letters of recommendation on college admissions decisions: Approximate balancing weights for treatment effect heterogeneity in observational studies.</strong> Eli Ben-Michael, Avi Feller, and Jesse Rothstein. <em>Annals of Applied Statistics.</em> 17(4): 2843–2864. [<a href="#" data-bs-target="#lor_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://dx.doi.org/10.1214/23-AOAS1740" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a class="text-decoration-none" target="_blank" href="https://arxiv.org/abs/2008.04394"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="lor_abstract" class="collapse">
<div class="card card-body">In a pilot program during the 2016-17 admissions cycle, the University of California, Berkeley invited many applicants for freshman admission to submit letters of recommendation. We use this pilot as the basis for an observational study of the impact of submitting letters of recommendation on subsequent admission, with the goal of estimating how impacts vary across pre-defined subgroups. Understanding this variation is challenging in observational studies, however, because estimated impacts reflect both actual treatment effect variation and differences in covariate balance across groups. To address this, we develop balancing weights that directly optimize for "local balance" within subgroups while maintaining global covariate balance between treated and control units. We then show that this approach has a dual representation as a form of inverse propensity score weighting with a hierarchical propensity score model. In the UC Berkeley pilot study, our proposed approach yields excellent local and global balance, unlike more traditional weighting methods, which fail to balance covariates within subgroups. We find that the impact of letters of recommendation increases with the predicted probability of admission, with mixed evidence of differences for under-represented minority applicants.
</div><br></div>
<p><strong>Is it who you are or where you are? Accounting for compositional differences in cross-site treatment effect variation.</strong> Benjamin Lu, Eli Ben-Michael, Avi Feller, and Luke Miratrix. <em>Journal of Educational and Behavioral Statistics.</em> 48(4): 420-453. [<a href="#" class="text-decoration-none" data-bs-toggle="collapse" data-bs-target="#multisite_abstract">abstract</a>; <a href="https://journals.sagepub.com/doi/10.3102/10769986231155427" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2103.14765" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div class="collapse" id="multisite_abstract">
<div class="card card-body">In multisite trials, learning about treatment effect variation across sites is critical for understanding where and for whom a program works. Unadjusted comparisons, however, capture "compositional" differences in the distributions of unit-level features as well as "contextual" differences in site-level features, including possible differences in program implementation. Our goal in this paper is to adjust site-level estimates for differences in the distribution of observed unit-level features by re-weighting (or "transporting") each site to a common distribution. This allows us to make "apples to apples" comparisons across sites, parcelling out the amount of cross-site effect variation explained by systematic differences in populations served. To do so we develop a framework for transporting effects using approximate balancing weights, where the weights are chosen to directly optimize unit-level covariate balance between each site and the target distribution. We first develop our approach for the general setting of transporting the effect of a single-site trial. We then extend our method to multisite trials, assess its performance via simulation, and use it to analyze a series of multisite trials of adult education and vocational training programs. In our application, we find that distributional differences are potentially masking cross-site variation. Our method is available in the balancer R package</div>
      <br>
</div>
<p><strong>Using supervised learning to estimate inequality in the size and persistence of income shocks.</strong> David Bruns-Smith, Avi Feller, and Emi Nakamura. <em>FAccT ’23: 2023 ACM Conference on Fairness, Accountability, and Transparency.</em> [<a href="#" data-bs-target="#income_facct_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://dl.acm.org/doi/10.1145/3593013.3594113" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="income_facct_abstract" class="collapse">
<div class="card card-body">Household responses to income shocks are important drivers of financial fragility, the evolution of wealth inequality, and the effectiveness of fiscal and monetary policy. Traditional approaches to measuring the size and persistence of income shocks are based on restrictive econometric models that impose strong homogeneity across households and over time. In this paper, we propose a more flexible, machine learning framework for estimating income shocks that allows for variation across all observable features and time horizons. First, we propose non-parametric estimands for shocks and shock persistence. We then show how to estimate these quantities by using off-the-shelf supervised learning tools to approximate the conditional expectation of future income given present information. We solve this income prediction problem in a large Icelandic administrative dataset, and then use the estimated shocks to document several features of labor income risk in Iceland that are not captured by standard economic income models.
</div>
<br>
</div>
<p><strong>Balancing weights for causal inference.</strong> Eric R. Cohn, Eli Ben-Michael, Avi Feller, and José Zubizarreta. <em>Handbook of Matching and Weighting Adjustments for Causal Inference</em> (Chapman and Hall/CRC). [<a href="#" data-bs-target="#balance_chapter_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.taylorfrancis.com/chapters/edit/10.1201/9781003102670-16/balancing-weights-causal-inference-eric-cohn-eli-ben-michael-avi-feller-jos%C3%A9-zubizarreta" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="balance_chapter_abstract" class="collapse">
<div class="card card-body">In this chapter, the authors introduce the balancing approach to weighting for covariate balance and causal inference. They begin by providing a framework for causal inference in observational studies, including typical assumptions necessary for the identification of average treatment effects. The authors motivate the task of finding weights that balance covariates and unify a variety of methods from the literature. It discusses several implementation and design choices for finding balancing weights in practice and discuss the trade-offs of these choices using an example from the canonical LaLonde data. An alternative approach for IPW weights instead directly targets the balancing property by seeking weights that balance covariates in the sample at hand. Arguably, the most important design choice in the balancing approach is the choice of the model class M over which imbalance is minimized. A common approach to estimating treatment effects in observational studies involves least squares linear regression.</div>
<br>
</div>
<p><strong>Multilevel calibration weighting for survey data.</strong> Eli Ben-Michael, Avi Feller, and Erin Hartman. <em>Political Analysis.</em> 1–19. [<a href="#" data-bs-target="#multical_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.cambridge.org/core/journals/political-analysis/article/multilevel-calibration-weighting-for-survey-data/CCB1183BA82E7589F4187DE23406C153" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a class="text-decoration-none" href="https://arxiv.org/abs/2102.09052" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="multical_abstract" class="collapse">
<div class="card card-body">In the November 2016 U.S. presidential election, many state-level public opinion polls, particularly in the Upper Midwest, incorrectly predicted the winning candidate. One leading explanation for this polling miss is that the precipitous decline in traditional polling response rates led to greater reliance on statistical methods to adjust for the corresponding bias—and that these methods failed to adjust for important interactions between key variables like educational attainment, race, and geographic region. Finding calibration weights that account for important interactions remains challenging with traditional survey methods: raking typically balances the margins alone, while post-stratification, which exactly balances all interactions, is only feasible for a small number of variables. In this paper, we propose multilevel calibration weighting, which enforces tight balance constraints for marginal balance and looser constraints for higher-order interactions. This incorporates some of the benefits of post-stratification while retaining the guarantees of raking. We then correct for the bias due to the relaxed constraints via a flexible outcome model; we call this approach "double regression with post-stratification." We use these tools to re-assess a large-scale survey of voter intention in the 2016 U.S. presidential election, finding meaningful gains from the proposed methods. The approach is available in the multical R package.
</div>
<br>
</div>
<p><strong>Estimating the effects of a California gun control program with Multitask Gaussian Processes.</strong> Eli Ben-Michael, David Arbour, Avi Feller, Alex Franks, and Steven Raphael. <em>Annals of Applied Statistics.</em> 17(2): 985-1016. [<a href="#" data-bs-target="#mtgp_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-17/issue-2/Estimating-the-effects-of-a-California-gun-control-program-with/10.1214/22-AOAS1654.short" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/2110.07006" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="mtgp_abstract" class="collapse">
<div class="card card-body">Gun violence is a critical public safety concern in the United States. In 2006, California implemented a unique firearm monitoring program, the Armed and Prohibited Persons System (APPS), to address gun violence in the state. The APPS program first identifies those firearm owners who become prohibited from owning one, due to federal or state law, then confiscates their firearms. Our goal is to assess the effect of APPS on California murder rates using annual, state-level crime data across the U.S. for the years before and after the introduction of the program. To do so, we adapt a nonparametric Bayesian approach, multitask Gaussian processes (MTGPs), to the panel data setting. MTGPs allow for flexible and parsimonious panel data models that nest many existing approaches and allow for direct control over both dependence across time and dependence across units as well as natural uncertainty quantification. We extend this approach to incorporate non-Normal outcomes, auxiliary covariates, and multiple outcome series, which are all important in our application. We also show that this approach has attractive Frequentist properties, including a representation as a weighting estimator with separate weights over units and time periods. Applying this approach, we find that the increased monitoring and enforcement from the APPS program substantially decreased homicides in California. We also find that the effect on murder is driven entirely by declines in gun-related murder with no measurable effect on non-gun murder. Estimated cost per murder avoided are substantially lower than conventional estimates of the value of a statistical life, suggesting a very high benefit-cost ratio for this enforcement effort.</div><br></div>
<p><strong>Hospital quality risk standardization via approximate balancing weights.</strong> Luke Keele, Eli Ben-Michael, Avi Feller, Rachel Kelz, and Luke Miratrix. <em>Annals of Applied Statistics.</em> 17(2): 901-928. [<a href="#" data-bs-target="#hospital_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-17/issue-2/Hospital-quality-risk-standardization-via-approximate-balancing-weights/10.1214/22-AOAS1629.short" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/2007.09056" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="hospital_abstract" class="collapse">
<div class="card card-body">Comparing outcomes across hospitals, often to identify underperforming hospitals, is a critical task in health services research. However, naive comparisons of average outcomes, such as surgery complication rates, can be misleading because hospital case mixes differ—a hospital’s overall complication rate may be lower simply because the hospital serves a healthier population overall. In this paper we develop a method of "direct standardization" where we reweight each hospital patient population to be representative of the overall population and then compare the weighted averages across hospitals. Adapting methods from survey sampling and causal inference, we find weights that directly control for imbalance between the hospital patient mix and the target population, even across many patient attributes. Critically, these balancing weights can also be tuned to preserve sample size for more precise estimates. We also derive principled measures of statistical uncertainty and use outcome modeling and Bayesian shrinkage to increase precision and account for variation in hospital size. We demonstrate these methods using claims data from Pennsylvania, Florida, and New York, estimating standardized hospital complication rates for general surgery patients. We conclude with a discussion of how to detect low performing hospitals.</div><br></div>
<h2 id="published-2022" class="anchored">2022</h2>
<p><strong>Weak Separation in Mixture Models and Implications for Principal Stratification.</strong> Nhat Ho, Avi Feller, Evan Greif, Luke Miratrix, and Natesh Pillai. <em>AISTATS: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics.</em> PMLR 151:5416-5458. [<a href="#twilight_zone_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://proceedings.mlr.press/v151/ho22b.html" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://www.arxiv.org/abs/1602.06595" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="twilight_zone_abstract" class="collapse">
<div class="card card-body">Principal stratification is a popular framework for addressing post-randomization complications, often in conjunction with finite mixture models for estimating the causal effects of interest. Unfortunately, standard estimators of mixture parameters, like the MLE, are known to exhibit pathological behavior. We study this behavior in a simple but fundamental example, a two-component Gaussian mixture model in which only the component means and variances are unknown, and focus on the setting in which the components are weakly separated. In this case, we show that the asymptotic convergence rate of the MLE is quite poor, such as O(n^{-1/6}) or even O(n^{-1/8}). We then demonstrate via both theoretical arguments and extensive simulations that the MLE behaves like a threshold estimator in finite samples, in the sense that the MLE can give strong evidence that the means are equal when the truth is otherwise. We also explore the behavior of the MLE when the MLE is non-zero, showing that it is difficult to estimate both the sign and magnitude of the means in this case. We provide diagnostics for all of these pathologies and apply these ideas to re-analyzing two randomized evaluations of job training programs, JOBS II and Job Corps. Our results suggest that the corresponding maximum likelihood estimates should be interpreted with caution in these cases.</div><br></div>
<p><strong>Outcome Assumptions and Duality Theory for Balancing Weights.</strong> David Bruns-Smith and Avi Feller. <em>AISTATS: Proceedings of The 25th International Conference on Artificial Intelligence and Statistics.</em> PMLR 151:11037-11055 [<a href="#" data-bs-target="#duality_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://proceedings.mlr.press/v151/bruns-smith22a.html" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/2203.09557" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="duality_abstract" class="collapse">
<div class="card card-body">We study balancing weight estimators, which reweight outcomes from a source population to estimate missing outcomes in a target population. These estimators minimize the worst-case error by making an assumption about the outcome model. In this paper, we show that this outcome assumption has two immediate implications. First, we can replace the minimax optimization problem for balancing weights with a simple convex loss over the assumed outcome function class. Second, we can replace the commonly-made overlap assumption with a more appropriate quantitative measure, the minimum worst-case bias. Finally, we show conditions under which the weights remain robust when our assumptions on the outcomes are wrong.</div><br></div>
<p><strong>Problems with evidence assessment in COVID-19 health policy impact evaluation: a systematic review of study design and evidence strength.</strong> Noah Haber, Emma Clark-Deelder, Avi Feller, et al.&nbsp;<em>BMJ Open.</em> 12(1), e053820. [<a href="#" data-bs-target="#covid_haber_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://bmjopen.bmj.com/content/12/1/e053820" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="covid_haber_abstract" class="collapse">
<div class="card card-body">
Introduction. Assessing the impact of COVID-19 policy is critical for informing future policies. However, there are concerns about the overall strength of COVID-19 impact evaluation studies given the circumstances for evaluation and concerns about the publication environment.
<br>
Methods. We included studies that were primarily designed to estimate the quantitative impact of one or more implemented COVID-19 policies on direct SARS-CoV-2 and COVID-19 outcomes. After searching PubMed for peer-reviewed articles published on 26 November 2020 or earlier and screening, all studies were reviewed by three reviewers first independently and then to consensus. The review tool was based on previously developed and released review guidance for COVID-19 policy impact evaluation.
<br>
Results. After 102 articles were identified as potentially meeting inclusion criteria, we identified 36 published articles that evaluated the quantitative impact of COVID-19 policies on direct COVID-19 outcomes. Nine studies were set aside because the study design was considered inappropriate for COVID-19 policy impact evaluation (n=8 pre/post; n=1 cross-sectional), and 27 articles were given a full consensus assessment. 20/27 met criteria for graphical display of data, 5/27 for functional form, 19/27 for timing between policy implementation and impact, and only 3/27 for concurrent changes to the outcomes. Only 4/27 were rated as overall appropriate. Including the 9 studies set aside, reviewers found that only four of the 36 identified published and peer-reviewed health policy impact evaluation studies passed a set of key design checks for identifying the causal impact of policies on COVID-19 outcomes.
<br>
Discussion. The reviewed literature directly evaluating the impact of COVID-19 policies largely failed to meet key design criteria for inference of sufficient rigour to be actionable by policy-makers. More reliable evidence review is needed to both identify and produce policy-actionable evidence, alongside the recognition that actionable evidence is often unlikely to be feasible.
</div><br></div>
<p><strong>A graph-theoretic approach to randomization tests of causal effects under general interference.</strong> David Puelz, Guillaume Basse, Avi Feller, and Panos Toulis. <em>Journal of the Royal Statistical Society (Series B)</em>. 84: 174-204. [<a href="#" data-bs-target="#clique_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12478" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1910.10862" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="clique_abstract" class="collapse">
<div class="card card-body">Measuring the effect of peers on individual outcomes is a challenging problem, in part because
individuals often select peers who are similar in both observable and unobservable ways. Group formation experiments avoid this problem by randomly assigning individuals to groups and observing their responses; for example, do first-year students have better grades when they are randomly assigned roommates who have stronger academic backgrounds? Standard approaches for analyzing these experiments, however, are heavily model-dependent and generally fail to exploit the randomized design. In this paper, we extend methods from randomization-based testing under interference to group formation experiments. The proposed tests are justified by the randomization itself, require relatively few assumptions, and are exact in finite samples. First, we develop procedures that yield valid tests for arbitrary group formation designs. Second, we derive sufficient conditions on the design such that the randomization test can be implemented via simple random permutations. We apply this approach to two recent group formation experiments.</div><br></div>
<h2 id="published-2021" class="anchored">2021</h2>
<p><strong>Synthetic controls under staggered adoption.</strong> Eli Ben-Michael, Avi Feller, and Jesse Rothstein. <em>Journal of the Royal Statistical Society (Series B)</em>. 84: 351–381. [<a href="#" data-bs-target="#multisynth_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://dx.doi.org/10.1111/rssb.12448" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1912.03290" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="multisynth_abstract" class="collapse">
<div class="card card-body">Staggered adoption of policies by different units at different times creates promising opportunities for observational causal inference. Estimation remains challenging, however, and common regression methods can give misleading results. A promising alternative is the synthetic control method (SCM), which finds a weighted average of control units that closely balances the treated unit's pre-treatment outcomes. In this paper, we generalize SCM, originally designed to study a single treated unit, to the staggered adoption setting. We first bound the error for the average effect and show that it depends on both the imbalance for each treated unit separately and the imbalance for the average of the treated units. We then propose "partially pooled" SCM weights to minimize a weighted combination of these measures; approaches that focus only on balancing one of the two components can lead to bias. We extend this approach to incorporate unit-level intercept shifts and auxiliary covariates. We assess the performance of the proposed method via extensive simulations and apply our results to the question of whether teacher collective bargaining leads to higher school spending, finding minimal impacts. We implement the proposed method in the augsynth R package.</div><br></div>
<p><strong>Challenges with evaluating education policy using panel data during and after the COVID-19 pandemic.</strong> Avi Feller and Elizabeth Stuart. <em>Journal of Research on Educational Effectiveness</em>. 2021. 14(3): 668–675. [<a href="#" data-bs-target="#covid_ed_panel_data_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.tandfonline.com/doi/full/10.1080/19345747.2021.1938316" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
        <div id="covid_ed_panel_data_abstract" class="collapse">
<div class="card card-body">Panel data methods, which include difference-in-differences and comparative interrupted time series, have become increasingly common in education policy research. The key idea is to use variation across time and space (e.g., school districts) to estimate the effects of policy or programmatic changes that happen in some localities but not others. In this commentary we highlight some specific challenges for panel or longitudinal studies of K-12 education interventions during and following the COVID-19 pandemic. Our goal is to help researchers think through the underlying issues and assumptions, and to help consumers of those studies assess their validity.</div><br></div>
<p><strong>The Augmented Synthetic Control Method.</strong> Eli Ben-Michael, Avi Feller, and Jesse Rothstein. <em>Journal of the American Statistical Association.</em> 116(536): 1789–1803. [<a href="#" data-bs-target="#synth_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1929245" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1811.04170" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
        <div id="synth_abstract" class="collapse">
<div class="card card-body">The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The "synthetic control" is a weighted average of control units that balances the treated unit's pre-treatment outcomes as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pre-treatment outcomes is excellent. We propose Augmented SCM to extend SCM to settings where such pre-treatment fit is infeasible. Analogous to bias correction for inexact matching, Augmented SCM uses an outcome model to estimate the bias due to imperfect pre-treatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pre-treatment fit while minimizing extrapolation from the convex hull. We bound the bias of this approach under a linear factor model and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package.</div><br></div>
<p><strong>Impact Evaluation of Coronavirus Disease 2019 Policy: A Guide to Common Design Issues.</strong> Noah Haber, Emma Clarke-Deelder, Joshua Salomon, Avi Feller, and Elizabeth Stuart. <em>American Journal of Epidemiology.</em> 190(11): 2474-2486. [<a href="#" data-bs-target="#covid_haber_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://academic.oup.com/aje/article/190/11/2474/6310594" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/2009.01940" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="covid_haber_abstract" class="collapse">
<div class="card card-body">Policy responses to COVID-19, particularly those related to non-pharmaceutical interventions, are unprecedented in scale and scope. Researchers and policymakers are striving to understand the impact of these policies on a variety of outcomes. Policy impact evaluations always require a complex combination of circumstance, study design, data, statistics, and analysis. Beyond the issues that are faced for any policy, evaluation of COVID-19 policies is complicated by additional challenges related to infectious disease dynamics and lags, lack of direct observation of key outcomes, and a multiplicity of interventions occurring on an accelerated time scale. In this paper, we (1) introduce the basic suite of policy impact evaluation designs for observational data, including cross-sectional analyses, pre/post, interrupted time-series, and difference-in-differences analysis, (2) demonstrate key ways in which the requirements and assumptions underlying these designs are often violated in the context of COVID-19, and (3) provide decision-makers and reviewers a conceptual and graphical guide to identifying these key violations. The overall goal of this paper is to help policy-makers, journal editors, journalists, researchers, and other research consumers understand and weigh the strengths and limitations of evidence that is essential to decision-making.
</div><br></div>
<p><strong>A trial emulation approach for policy evaluations with group-level longitudinal data.</strong> Eli Ben-Michael, Avi Feller, and Elizabeth Stuart. <em>Epidemiology.</em> 32(4): 533–540. [<a href="#" data-bs-target="#epi_trial_emulation_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://journals.lww.com/epidem/Abstract/2021/07000/A_Trial_Emulation_Approach_for_Policy_Evaluations.10.aspx" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/2011.05826" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="epi_trial_emulation_abstract" class="collapse">
<div class="card card-body">To limit the spread of the novel coronavirus, governments across the world implemented extraordinary physical distancing policies, such as stay-at-home orders, and numerous studies aim to estimate their effects. Many statistical and econometric methods, such as difference-in-differences, leverage repeated measurements and variation in timing to estimate policy effects, including in the COVID-19 context. While these methods are less common in epidemiology, epidemiologic researchers are well accustomed to handling similar complexities in studies of individual-level interventions. "Target trial emulation" emphasizes the need to carefully design a non-experimental study in terms of inclusion and exclusion criteria, covariates, exposure definition, and outcome measurement -- and the timing of those variables. We argue that policy evaluations using group-level longitudinal ("panel") data need to take a similar careful approach to study design, which we refer to as "policy trial emulation." This is especially important when intervention timing varies across jurisdictions; the main idea is to construct target trials separately for each "treatment cohort" (states that implement the policy at the same time) and then aggregate. We present a stylized analysis of the impact of state-level stay-at-home orders on total coronavirus cases. We argue that estimates from panel methods -- with the right data and careful modeling and diagnostics -- can help add to our understanding of many policies, though doing so is often challenging.</div><br></div>
<p><strong>Overlap in observational studies with high-dimensional covariates.</strong> Alex D’Amour, Peng Ding, Avi Feller, Lihua Lei, and Jasjeet Sekhon. <em>Journal of Econometrics.</em> 221: 644-654. [<a href="#" data-bs-target="#overlap_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.sciencedirect.com/science/article/pii/S0304407620302694" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="overlap_abstract" class="collapse"><div class="card card-body">Causal inference in observational settings typically rests on a pair of identifying assumptions: (1) unconfoundedness and (2) covariate overlap, also known as positivity or common support. Investigators often argue that unconfoundedness is more plausible when many covariates are included in the analysis. Less discussed is the fact that covariate overlap is more difficult to satisfy in this setting. In this paper, we explore the implications of overlap in high-dimensional observational studies, arguing that this assumption is stronger than investigators likely realize. Our main innovation is to frame (strict) overlap in terms of bounds on a likelihood ratio, which allows us to leverage and expand on existing results from information theory. In particular, we show that strict overlap bounds discriminating information (e.g., Kullback-Leibler divergence) between the covariate distributions in the treated and control populations. We use these results to derive explicit bounds on the average imbalance in covariate means under strict overlap and a range of assumptions on the covariate distributions. Importantly, these bounds grow tighter as the dimension grows large, and converge to zero in some cases. We examine how restrictions on the treatment assignment and outcome processes can weaken the implications of certain overlap assumptions, but at the cost of stronger requirements for unconfoundedness. Taken together, our results suggest that adjusting for high-dimensional covariates does not necessarily make causal identification more plausible.</div><br></div>
<h2 id="published-2020" class="anchored">2020</h2>
<p><strong>Flexible sensitivity analysis for observational studies without observable implications.</strong> Alex Franks, Alex D’Amour, and Avi Feller. <em>Journal of the American Statistical Association.</em> 115(532): 1730-1746. [<a href="#" data-bs-target="#sensitivity_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.tandfonline.com/doi/10.1080/01621459.2019.1604369" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1809.00399" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="sensitivity_abstract" class="collapse">
<div class="card card-body">A fundamental challenge in observational causal inference is that assumptions about unconfoundedness are not testable from data. Assessing sensitivity to such assumptions is therefore important in practice. Unfortunately, some existing sensitivity analysis approaches inadvertently impose restrictions that are at odds with modern causal inference methods, which emphasize flexible models for observed data. To address this issue, we propose a framework that allows (1) flexible models for the observed data and (2) clean separation of the identified and unidentified parts of the sensitivity model. Our framework extends an approach from the missing data literature, known as Tukey’s factorization, to the causal inference setting. Under this factorization, we can represent the distributions of unobserved potential outcomes in terms of unidentified selection functions that posit a relationship between treatment assignment and unobserved potential outcomes. The sensitivity parameters in this framework are easily interpreted, and we provide heuristics for calibrating these parameters against observable quantities. We demonstrate the flexibility of this approach in two examples, where we estimate both average treatment effects and quantile treatment effects using Bayesian nonparametric models for the observed data.</div><br></div>
<p><strong>Bayesian sensitivity analysis for offline policy evaluation.</strong> Jongbin Jung, Ravi Shroff, Avi Feller, and Sharad Goel. <em>AIES: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.</em> 64–70. [<a href="#" data-bs-target="#bsa_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://dl.acm.org/doi/10.1145/3375627.3375822" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1805.01868" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="bsa_abstract" class="collapse">
<div class="card card-body">On a variety of complex decision-making tasks, from doctors prescribing treatment to judges setting bail, machine learning algorithms have been shown to outperform expert human judgments. One complication, however, is that it is often difficult to anticipate the effects of algorithmic policies prior to deployment, as one generally cannot use historical data to directly observe what would have happened had the actions recommended by the algorithm been taken. A common strategy is to model potential outcomes for alternative decisions assuming that there are no unmeasured confounders (i.e., to assume ignorability). But if this ignorability assumption is violated, the predicted and actual effects of an algorithmic policy can diverge sharply. In this paper we present a flexible Bayesian approach to gauge the sensitivity of predicted policy outcomes to unmeasured confounders. In particular, and in contrast to past work, our modeling framework easily enables confounders to vary with the observed covariates. We demonstrate the efficacy of our method on a large dataset of judicial actions, in which one must decide whether defendants awaiting trial should be required to pay bail or can be released without payment.</div><br></div>
<h2 id="published-2019" class="anchored">2019</h2>
<p><strong>Assessing treatment effect variation in observational studies: Results from a data challenge.</strong> Carlos Carvalho, Avi Feller, Jared Murray, Spencer Woody, David Yeager. <em>Observational Studies.</em> 5: 21-35. [<a href="#" data-bs-target="#acic_data_challenge_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://muse.jhu.edu/article/793355/summary" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1907.07592" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="acic_data_challenge_abstract" class="collapse">
<div class="card card-body">A growing number of methods aim to assess the challenging question of treatment effect variation in observational studies. This special section of "Observational Studies" reports the results of a workshop conducted at the 2018 Atlantic Causal Inference Conference designed to understand the similarities and differences across these methods. We invited eight groups of researchers to analyze a synthetic observational data set that was generated using a recent large-scale randomized trial in education. Overall, participants employed a diverse set of methods, ranging from matching and flexible outcome modeling to semiparametric estimation and ensemble approaches. While there was broad consensus on the topline estimate, there were also large differences in estimated treatment effect moderation. This highlights the fact that estimating varying treatment effects in observational studies is often more challenging than estimating the average treatment effect alone. We suggest several directions for future work arising from this workshop.</div><br></div>
<p><strong>Identifying and estimating principal causal effects in a multi-site trial of Early College High Schools.</strong> Lo-Hua Yuan, Avi Feller, and Luke Miratrix. <em>Annals of Applied Statistics.</em> 13(3): 1348-1369. [<a href="#" data-bs-target="#multisite_pce_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://projecteuclid.org/euclid.aoas/1571277756" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1803.06048" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="multisite_pce_abstract" class="collapse">
<div class="card card-body">Randomized trials are often conducted with separate randomizations across multiple sites such as schools, voting districts, or hospitals. These sites can differ in important ways, including the site’s implementation quality, local conditions, and the composition of individuals. An important question in practice is whether---and under what assumptions---researchers can leverage this cross-site variation to learn more about the intervention. We address these questions in the principal stratification framework, which describes causal effects for subgroups defined by post-treatment quantities. We show that researchers can estimate certain principal causal effects via the multi-site design if they are willing to impose the strong assumption that the site-specific effects are independent of the site-specific distribution of stratum membership. We motivate this approach with a multi-site trial of the Early College High School Initiative, a unique secondary education program with the goal of increasing high school graduation rates and college enrollment. Our analyses corroborate previous studies suggesting that the initiative had positive effects for students who would have otherwise attended a low-quality high school, although power is limited.</div><br></div>
<p><strong>Exact conditional randomization tests for causal effects under interference.</strong> Guillaume Basse, Avi Feller, and Panos Toulis. <em>Biometrika.</em> 106(2): 487-494. [<a href="#" data-bs-target="#exact_tests_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://academic.oup.com/biomet/article/106/2/487/5306899" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1709.08036" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="exact_tests_abstract" class="collapse">
<div class="card card-body">Many important causal questions involve interactions between units, also known as interference, such as interactions between individuals in households, students in schools, and firms in markets. Standard methods often break down in this setting. Permuting individual-level treatment assignments, for example, does not generally permute the treatment exposures of interest, such as spillovers, which depend on both the treatment assignment and the interference structure. One approach is to restrict the randomization test to a subset of units and assignments such that permuting the treatment assignment vector also permutes the treatment exposures, thus emulating the classical Fisher randomization test under no interference. Existing tests, however, can only leverage limited information in the structure of interference, which can lead to meaningful loss in power and introduce computational challenges. In this paper, we introduce the concept of a conditioning mechanism, which provides a framework for constructing valid and powerful randomization tests under general forms of interference. We describe our framework in the context of two-stage randomized designs and apply this approach to an analysis of a randomized evaluation of an intervention targeting student absenteeism in the School District of Philadelphia. We show meaningful improvements over existing methods, both in terms of computation and statistical power.</div><br></div>
<p><strong>Decomposing treatment effect variation.</strong> Peng Ding, Avi Feller, and Luke Miratrix. <em>Journal of the American Statistical Association.</em> 114(525): 304-317. [<a href="#" data-bs-target="#decomp_het_tx_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1407322" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="http://arxiv.org/abs/1605.06566" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="decomp_het_tx_abstract" class="collapse"><div class="card card-body">Understanding and characterizing treatment effect variation in randomized experiments has become essential for going beyond the "black box" of the average treatment effect. Nonetheless, traditional statistical approaches often ignore or assume away such variation. In the context of a randomized experiment, this paper proposes a framework for decomposing overall treatment effect variation into a systematic component that is explained by observed covariates, and a remaining idiosyncratic component. Our framework is fully randomization-based, with estimates of treatment effect variation that are fully justified by the randomization itself. Our framework can also account for noncompliance, which is an important practical complication. We make several key contributions. First, we show that randomization-based estimates of systematic variation are very similar in form to estimates from fully-interacted linear regression and two stage least squares. Second, we use these estimators to develop an omnibus test for systematic treatment effect variation, both with and without noncompliance. Third, we propose an R2-like measure of treatment effect variation explained by covariates and, when applicable, noncompliance. Finally, we assess these methods via simulation studies and apply them to the Head Start Impact Study, a large-scale randomized experiment.</div><br></div>
<h2 id="published-2018" class="anchored">2018</h2>
<p><strong>Analyzing two-stage experiments in the presence of interference.</strong> Guillaume Basse and Avi Feller. <em>Journal of the American Statistical Association.</em> 113(521): 41-55. [<a href="#" data-bs-target="#multilevel_estimation_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1323641" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="http://arxiv.org/abs/1608.06805" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="multilevel_estimation_abstract" class="collapse">
<div class="card card-body">Two-stage randomization is a powerful design for estimating treatment effects in the presence of interference; that is, when one individual's treatment assignment affects another individual's outcomes. Our motivating example is a two-stage randomized trial evaluating an intervention to reduce student absenteeism in the School District of Philadelphia. In that experiment, households with multiple students were first assigned to treatment or control; then, in treated households, one student was randomly assigned to treatment. Using this example, we highlight key considerations for analyzing two-stage experiments in practice. Our first contribution is to address additional complexities that arise when household sizes vary; in this case, researchers must decide between assigning equal weight to households or equal weight to individuals. We propose unbiased estimators for a broad class of individual- and household-weighted estimands, with corresponding theoretical and estimated variances. Our second contribution is to connect two common approaches for analyzing two-stage designs: linear regression and randomization inference. We show that, with suitably chosen standard errors, these two approaches yield identical point and variance estimates, which is somewhat surprising given the complex randomization scheme. Finally, we explore options for incorporating covariates to improve precision. We confirm our analytic results via simulation studies and apply these methods to the attendance study, finding substantively meaningful spillover effects.</div><br></div>
<p><strong>Reducing absences at scale by targeting parents’ misbeliefs.</strong> Todd Rogers and Avi Feller. <em>Nature Human Behavior.</em> 2(5): 335-342. [<a href="#" data-bs-target="#absenteeism_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.nature.com/articles/s41562-018-0328-1" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://scholar.harvard.edu/files/todd_rogers/files/sdp_revision.10.30.2017_final.pdf" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="absenteeism_abstract" class="collapse"><div class="card card-body">Student attendance is critical to educational success, and is increasingly the focus of educators, researchers, and policymakers. We report the first randomized experiment examining interventions targeting student absenteeism (N=28,080). Parents of high-risk, K-12 students received one of three personalized information treatments repeatedly throughout the school year. The most effective versions reduced chronic absenteeism by 10%, partly by correcting parents' biased beliefs about their students' total absences. The intervention reduced student absences comparably across grade levels, and reduced absences among untreated cohabiting students in treated households. This intervention is easy to scale and is more than an order of magnitude more cost effective than current absence-reduction best practices. Educational interventions that inform and empower parents, like those reported here, can complement more intensive student-focused absenteeism interventions.</div><br></div>
<p><strong>Information, knowledge and attitudes: An Evaluation of the taxpayer receipt.</strong> Lucy Barnes, Avi Feller, Jake Haselswerdt, and Ethan Porter. <em>Journal of Politics.</em> 80(2): 701-706. [<a href="#" data-bs-target="#taxpayer_receipt_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.journals.uchicago.edu/doi/abs/10.1086/695672" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2877248" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>] <br> Media: <a href="https://www.washingtonpost.com/news/monkey-cage/wp/2015/11/24/where-do-your-taxes-go-this-month-u-k-citizens-get-the-answer-in-the-mail/" class="text-decoration-none" target="_blank"><em>Washington Post</em> op-ed</a></p>
<div id="taxpayer_receipt_abstract" class="collapse"><div class="card card-body">To better understand the relationship between information and political knowledge, we evaluate an ambitious government initiative: the nationwide dissemination of "taxpayer receipts," or personalized, itemized accounts of government spending, by the UK government in Fall 2014. In coordination with the British tax authorities, we embedded a survey experiment in a nationally representative panel. We find that citizens became more knowledgeable about government spending because of our encouragement to read their receipt. Although baseline levels of political knowledge are indeed low, our findings indicate that individuals are capable of learning and retaining complex political information. However, even as citizens became more knowledgeable, we uncover no evidence that their attitudes toward government and redistribution changed concomitantly. The acquisition and retention of new information does not necessarily change attitudes. Our results have implications for citizens' capacity to learn and research on the relationship between knowledge and attitudes.</div><br></div>
<p><strong>New Findings on Impact Variation from the Head Start Impact Study: Informing the Scale-up of Early Childhood Programs.</strong> Pamela Morris, Maia Connors, Allison Freidman-Krauss, Dana McCoy, Avi Feller, Lindsay Page, Howard Bloom, Hirokazu Yoshikawa. <em>AERA Open.</em> 4(2): 1-16. [<a href="#" data-bs-target="#aera_head_start_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://journals.sagepub.com/doi/full/10.1177/2332858418769287" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="aera_head_start_abstract" class="collapse"><div class="card card-body">This article synthesizes findings from a reanalysis of data from the Head Start Impact Study with a focus on impact variation. This study addressed whether the size of Head Start's impacts on children’s access to center-based and high-quality care and their school readiness skills varied by child characteristics, geographic location, and the experiences of children in the control group. Across multiple sets of analyses based on new, innovative statistical methods, findings suggest that the topline Head Start Impact Study results of Head Start's average impacts mask substantial variation in its effectiveness and that one key source of that variation was in the counterfactual experiences and the context of Head Start sites (as well as the more typically examined child characteristics; e.g., children's dual language learner status). Implications are discussed for the future of Head Start and further research, as well as the scale-up of other early childhood programs, policies, and practices.</div><br></div>
<p><strong>The Millennium Villages Project: a retrospective observational end-line evaluation.</strong> Shira Mitchell, Andrew Gelman, Rebecca Ross, Joyce Chen, Sehrish Bari, Uyen Kim Huynh, Matthew Harris, Sonia Sachs, Elizabeth Stuart, Avi Feller, Susanna Makela, Alan Zaslavsky, Lucy McClellan, Seth Ohemeng-Dapaah, Patricia Namakula, Cheryl Palm, and Jeffrey Sachs. <em>Lancet Global Health.</em> 6(5): e500–e513. [<a href="#" data-bs-target="#lancet_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30065-2/fulltext" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="lancet_abstract" class="collapse"><div class="card card-body">
Background: The Millennium Villages Project (MVP) was a ten-year, multi-sector, rural development project implemented in ten sites in ten sub-Saharan African countries to achieve the Millennium Development Goals (MDGs). This paper summarizes statistical analyses of survey and
on-site spending data for the end-line evaluation of the MVP, including estimates of (1) project impacts, (2) MDG and project-specified target attainment, and (3) on-site spending.
<br>
Methods: To estimate project impacts, we retrospectively selected comparison villages, matching the project villages on possible confounding variables. At end-line, we collected cross-sectional survey data in both the project and comparison villages. Using these data, as well as survey and on-site spending data collected in the project villages during implementation, we estimated: (1) project impacts as differences in outcomes between project and matched comparison villages; (2) target attainment as differences between project outcomes and pre-specified targets; and (3) on-site spending as reported expenditures by communities, donors, governments, and the project (on-site).
<br>
Findings: Averaged across the ten project sites, we found that: (1) impact estimates on 30 out of 40 outcomes met the conventional criterion of statistical significance (p-value &lt; 0.05). On all 30, estimated impacts were favorable. We found particularly substantial impacts in agriculture and health, in which some project outcomes were roughly one standard deviation better than their comparisons. (2) One-third of the targets were met in the project sites. (3) Total on-site spending decreased from $132 per capita in the first half to $109 per capita in the second half of the project.
<br>
Interpretation: Of all the sectors, the strongest estimated impacts were on health outcomes, suggesting support for the project's health systems strengthening approach.</div><br></div>
<h2 id="published-2017" class="anchored">2017</h2>
<p><strong>Bounding, an accessible method for estimating principal causal effects, examined and explained.</strong> Luke Miratrix, Jane Furey, Avi Feller, Todd Grindal, and Lindsay Page. <em>Journal of Research on Educational Effectiveness.</em> 11(1): 133-162. [<a href="#" data-bs-target="#bounds_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://www.tandfonline.com/doi/abs/10.1080/19345747.2017.1379576" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1701.03139" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="bounds_abstract" class="collapse"><div class="card card-body">Estimating treatment effects for subgroups defined by post-treatment behavior (i.e., estimating causal effects in a principal stratification framework) can be technically challenging and heavily reliant on strong assumptions. We investigate an alternate path: using bounds to identify ranges of possible effects that are consistent with the data. This simple approach relies on weak assumptions yet can result in policy-relevant findings. Further, covariates can be used to sharpen bounds, as we show. Via simulation, we demonstrate which types of covariates are maximally beneficial. We conclude with an analysis of a multi-site experimental study of Early College High Schools. When examining the program's impact on students completing the ninth grade "on-track" for college, we find little impact for ECHS students who would otherwise attend a high quality high school, but substantial effects for those who would not. This suggests potential benefit in expanding these programs in areas primarily served by lower quality schools.</div><br></div>
<p><strong>Principal score methods: Assumptions, extensions, and practical considerations.</strong> Avi Feller, Fabrizia Mealli, and Luke Miratrix. <em>Journal of Educational and Behavioral Statistics.</em> 42(6): 726-758. [<a href="#" data-bs-target="#princ_score_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://journals.sagepub.com/doi/abs/10.3102/1076998617719726" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="http://arxiv.org/abs/1606.02682" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="princ_score_abstract" class="collapse">
<div class="card card-body">Researchers addressing post-treatment complications in randomized trials often turn to principal stratification to define relevant assumptions and quantities of interest. One approach for the subsequent estimation of causal effects in this framework is to use methods based on the "principal score," the conditional probability of belonging to a certain principal stratum given covariates. These methods typically assume that stratum membership is as good as randomly assigned given these covariates. We clarify the key assumption in this context, known as Principal Ignorability, and argue that versions of this assumption are quite strong in practice. We describe these concepts in terms of both one- and two-sided noncompliance and propose a novel approach for researchers to "mix and match" Principal Ignorability assumptions with alternative assumptions, such as the exclusion restriction. Finally, we apply these ideas to a randomized evaluation of a job training program and a randomized evaluation of an early childhood education program. Overall, applied researchers should acknowledge that principal score methods, while useful tools, rely on assumptions that are typically hard to justify in practice.</div><br></div>
<p><strong>Algorithmic decision making and the cost of fairness.</strong> Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. <em>KDD: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</em> [<a href="#" data-bs-target="#algo_bias_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://5harad.com/papers/fairness.pdf" target="_blank" class="text-decoration-none"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>] <br>Media: <a href="https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/?utm_term=.2ea33aca1bc2" target="_blank" class="text-decoration-none"><em>Washington Post</em> op-ed</a></p>
<div id="algo_bias_abstract" class="collapse"><div class="card card-body">Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community.
In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques recently have been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of their race. Because the optimal constrained and unconstrained algorithms in general differ, reducing racial disparities is at odds with improving public safety. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on the problem of designing algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.</div><br></div>
<h2 id="published-2016" class="anchored">2016</h2>
<p><strong>Compared to what? Variation in the impacts of early childhood education by alternative care type.</strong> Avi Feller, Todd Grindal, Luke Miratrix, and Lindsay Page. <em>Annals of Applied Statistics.</em> 2016. 110(3): 1245-1285. [<a href="#" data-bs-target="#hsis_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://projecteuclid.org/euclid.aoas/1475069607" target="_blank" class="text-decoration-none"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>] <br> Media: <a href="http://www.huffingtonpost.com/todd-grindal/when-it-comes-to-publicly_b_10920076.html" target="_blank" class="text-decoration-none">Huffington Post op-ed</a>; <a href="http://irle.berkeley.edu/policybriefs/IRLE-Revisiting-the-impact-of-Head-Start.pdf" target="_blank" class="text-decoration-none">IRLE brief on Head Start research at UC Berkeley</a></p>
<div id="hsis_abstract" class="collapse"><div class="card card-body">Early childhood education research often compares a group of children who receive the intervention of interest to a group of children who receive care in a range of different care settings. In this paper, we estimate differential impacts of an early childhood intervention by alternative care setting, using data from the Head Start Impact Study, a large-scale randomized evaluation. To do so, we utilize a Bayesian principal stratification framework to estimate separate impacts for two types of Compliers: those children who would otherwise be in other center-based care when assigned to control and those who would otherwise be in home-based care. We find strong, positive short-term effects of Head Start on receptive vocabulary for those Compliers who would otherwise be in home-based care. By contrast, we find no meaningful impact of Head Start on vocabulary for those Compliers who would otherwise be in other center-based care. Our findings suggest that alternative care type is a potentially important source of variation in early childhood education interventions.</div><br></div>
<p><strong>Randomization inference for treatment effect variation.</strong> Peng Ding, Avi Feller, and Luke Miratrix. <em>Journal of the Royal Statistical Society (Series B).</em> 78(3): 655-671. [<a href="#" data-bs-target="#jrssb_het_tx_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12124" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>; <a href="https://arxiv.org/abs/1412.5000" class="text-decoration-none" target="_blank"><i class="bi bi-file-earmark-pdf" role="img" aria-label="pre-print"></i> pre-print</a>]</p>
<div id="jrssb_het_tx_abstract" class="collapse"><div class="card card-body">Applied researchers are increasingly interested in whether and how treatment effects vary in randomized evaluations, especially variation that is not explained by observed covariates. We propose a model-free approach for testing for the presence of such unexplained variation. To use this randomization-based approach, we must address the fact that the average treatment effect, which is generally the object of interest in randomized experiments, actually acts as a nuisance parameter in this setting. We explore potential solutions and advocate for a method that guarantees valid tests in finite samples despite this nuisance. We also show how this method readily extends to testing for heterogeneity beyond a given model, which can be useful for assessing the sufficiency of a given scientific theory. We finally apply our method to the National Head Start impact study, which is a large-scale randomized evaluation of a Federal preschool programme, finding that there is indeed significant unexplained treatment effect variation.</div><br></div>
<p><strong>Discouraged by peer excellence: Exposure to exemplary peer performance causes quitting.</strong> Todd Rogers and Avi Feller. <em>Psychological Science.</em> 27(3): 365-374. [<a href="#" data-bs-target="#peer_excellence_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://pss.sagepub.com/content/27/3/365.short" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>] <br>Media: <a class="text-decoration-none" target="_blank" href="http://www.npr.org/2016/03/25/471817328/peer-pressure-may-not-work-the-way-we-think-it-does">NPR Hidden Brain/Science Friday</a>, <a class="text-decoration-none" target="_blank" href="http://www.edweek.org/ew/articles/2016/02/17/study-showing-students-standout-work-can-backfire.html">Education Week</a>, <a class="text-decoration-none" target="_blank" href="https://www.sciencedaily.com/releases/2016/02/160202143624.htm">Science Daily</a></p>
<div id="peer_excellence_abstract" class="collapse"><div class="card card-body">People are exposed to exemplary peer performances often (and sometimes by design in interventions). In two studies, we showed that exposure to exemplary peer performances can undermine motivation and success by causing people to perceive that they cannot attain their peers’ high levels of performance. It also causes de-identification with the relevant domain. We examined such discouragement by peer excellence by exploiting the incidental exposure to peers’ abilities that occurs when students are asked to assess each other’s work. Study 1 was a natural experiment in a massive open online course that employed peer assessment (N = 5,740). Exposure to exemplary peer performances caused a large proportion of students to quit the course. Study 2 explored underlying psychological mechanisms in an online replication (N = 361). Discouragement by peer excellence has theoretical implications for work on social judgment, social comparison, and reference bias and has practical implications for interventions that induce social comparisons.</div><br></div>
<h2 id="published-2015" class="anchored">2015 and earlier</h2>
<p><strong>Principal Stratification: A tool for understanding variation in program effects across endogenous subgroups.</strong> Lindsay Page, Avi Feller, Todd Grindal, Luke Miratrix, and Marie-Andree Somers. <em>American Journal of Evaluation.</em> 2015. 36(4): 514-531. [<a href="#" data-bs-target="#aje_ps_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://aje.sagepub.com/content/36/4/514" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="aje_ps_abstract" class="collapse"><div class="card card-body">Increasingly, researchers are interested in questions regarding treatment-effect variation across partially or fully latent subgroups defined not by pretreatment characteristics but by postrandomization actions. One promising approach to address such questions is principal stratification. Under this framework, a researcher defines endogenous subgroups, or principal strata, based on post-randomization behaviors under both the observed and the counterfactual experimental conditions. These principal strata give structure to such research questions and provide a framework for determining estimation strategies to obtain desired effect estimates. This article provides a nontechnical primer to principal stratification. We review selected applications to highlight the breadth of substantive questions and methodological issues that this method can inform. We then discuss its relationship to instrumental variables analysis to address binary noncompliance in an experimental context and highlight how the framework can be generalized to handle more complex posttreatment patterns. We emphasize the counterfactual logic fundamental to principal stratification and the key assumptions that render analytic challenges more tractable. We briefly discuss technical aspects of estimation procedures, providing a short guide for interested readers.</div><br></div>
<p><strong>Hierarchical models for causal effects.</strong> Avi Feller and Andrew Gelman. <em>Emerging Trends in the Social and Behavioral Sciences.</em> 2015. [<a href="#" data-bs-target="#hierarchical_models_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118900772.etrds0160/abstract" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="hierarchical_models_abstract" class="collapse"><div class="card card-body">Hierarchical models play three important roles in modeling causal effects: (i) accounting for data collection, such as in stratified and split-plot experimental designs; (ii) adjusting for unmeasured covariates, such as in panel studies; and (iii) capturing treatment effect variation, such as in subgroup analyses. Across all three areas, hierarchical models, especially Bayesian hierarchical modeling, offer substantial benefits over classical, non-hierarchical approaches. After discussing each of these topics, we explore some recent developments in the use of hierarchical models for causal inference and conclude with some thoughts on new directions for this research area.</div><br></div>
<p><strong>Genome-wide profiling of chromosome interactions in Plasmodium falciparum characterizes nuclear architecture and reconfigurations associated with antigenic variation.</strong> Jacob Lemieux, Sue Kyes, Thomas Otto, Avi Feller, Richard Eastman, Robert Pinches, Matthew Berriman, Xin-zhuan Su, Chris Newbold. <em>Molecular microbiology.</em> 2013. 90(3): 519–537. [<a href="#" data-bs-target="#mol_micro_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://onlinelibrary.wiley.com/doi/10.1111/mmi.12381/full" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="mol_micro_abstract" class="collapse"><div class="card card-body">Spatial relationships within the eukaryotic nucleus are essential for proper nuclear function. In Plasmodium falciparum, the repositioning of chromosomes has been implicated in the regulation of the expression of genes responsible for antigenic variation, and the formation of a single, peri-nuclear nucleolus results in the clustering of rDNA. Nevertheless, the precise spatial relationships between chromosomes remain poorly understood, because, until recently, techniques with sufficient resolution have been lacking. Here we have used chromosome conformation capture and second-generation sequencing to study changes in chromosome folding and spatial positioning that occur during switches in var gene expression. We have generated maps of chromosomal spatial affinities within the P. falciparum nucleus at 25 Kb resolution, revealing a structured nucleolus, an absence of chromosome territories, and confirming previously identified clustering of heterochromatin foci. We show that switches in var gene expression do not appear to involve interaction with a distant enhancer, but do result in local changes at the active locus. These maps reveal the folding properties of malaria chromosomes, validate known physical associations, and characterize the global landscape of spatial interactions. Collectively, our data provide critical information for a better understanding of gene expression regulation and antigenic variation in malaria parasites.</div><br></div>
<p><strong>Red state/blue state divisions in the 2012 presidential election.</strong> Avi Feller, Andrew Gelman, and Boris Shor. <em>The Forum.</em> 2013. 10(4): 127–131. [<a href="#" data-bs-target="#the_forum_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://www.degruyter.com/view/j/for.2012.10.issue-4/forum-2013-0014/forum-2013-0014.xml" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>] <br><a class="text-decoration-none" target="_blank" href="http://campaignstops.blogs.nytimes.com/2012/11/12/red-versus-blue-in-a-new-light/?_r=0"><em>New York Times</em> version</a>: “Red versus blue in a new light.” Nov.&nbsp;12, 2012. with Andrew Gelman.</p>
<div id="the_forum_abstract" class="collapse"><div class="card card-body">The so-called "red/blue paradox" is that rich individuals are more likely to vote Republican but rich states are more likely to support the Democrats. Previous research argued that this seeming paradox could be explained by comparing rich and poor voters within each state – the difference in the Republican vote share between rich and poor voters was much larger in low-income, conservative, middle-American states like Mississippi than in high-income, liberal, coastal states like Connecticut. We use exit poll and other survey data to assess whether this was still the case for the 2012 Presidential election. Based on this preliminary analysis, we find that, while the red/blue paradox is still strong, the explanation offered by Gelman et al. no longer appears to hold. We explore several empirical patterns from this election and suggest possible avenues for resolving the questions posed by the new data.</div><br></div>
<p><strong>Genome wide adaptations of <em>Plasmodium falciparum</em> in response to lumefantrine selective drug pressure.</strong> Leah Mwai, Abdi Diriye, Victor Masseno, Steven Muriithi, Theresa Feltwell, Jennifer Musyoki, Jacob Lemieux, Avi Feller, Gunnar Mair, Kevin Marsh, Chris Newbold, Alexis Nzila, Celine Carret. <em>PloS One.</em> 2012. 7(2): e31623. [<a href="#" data-bs-target="#plos_one_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0031623" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="plos_one_abstract" class="collapse"><div class="card card-body">The combination therapy of the Artemisinin-derivative Artemether (ART) with Lumefantrine (LM) (CoartemH) is an important malaria treatment regimen in many endemic countries. Resistance to Artemisinin has already been reported, and it is feared that LM resistance (LMR) could also evolve quickly. Therefore molecular markers which can be used to track CoartemH efficacy are urgently needed. Often, stable resistance arises from initial, unstable phenotypes that can be identified in vitro. Here we have used the Plasmodium falciparum multidrug resistant reference strain V1S to induce LMR in vitro by culturing the parasite under continuous drug pressure for 16 months. The initial IC50 (inhibitory concentration that kills 50% of the parasite population) was 24 nM. The resulting resistant strain V1SLM, obtained after culture for an estimated 166 cycles under LM pressure, grew steadily in 378 nM of LM, corresponding to 15 times the IC50 of the parental strain. However, after two weeks of culturing V1SLM in drug-free medium, the IC50 returned to that of the initial, parental strain V1S. This transient drug tolerance was associated with major changes in gene expression profiles: using the PFSANGER Affymetrix custom array, we identified 184 differentially expressed genes in V1SLM. Among those are 18 known and putative transporters including the multidrug resistance gene 1 (pfmdr1), the multidrug resistance associated protein and the V-type H+ pumping pyrophosphatase 2 (pfvp2) as well as genes associated with fatty acid metabolism. In addition we detected a clear selective advantage provided by two genomic loci in parasites grown under LM drug pressure, suggesting that all, or some of those genes contribute to development of LM tolerance—they may prove useful as molecular markers to monitor P. falciparum LM susceptibility.</div><br></div>
<p><strong><em>In vivo</em> profiles show continuous variation between 2 cellular populations.</strong> Jacob Lemieux*, Avi Feller*, Chris Holmes, and Chris Newbold (* indicates equal contribution). <em>Proceedings of the National Academy of Sciences.</em> 2009. 106 (27), E71–E72. [<a href="#" data-bs-target="#pnas_2_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://www.pnas.org/content/106/27/E71.short" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="pnas_2_abstract" class="collapse"><div class="card card-body">In this reply, we argue that seemingly discrete variation in the authors' published gene expression profiles is actually a combination of continuous variation and technical biological errors in the original experiment.</div><br></div>
<p><strong>Statistical estimation of cell-cycle progression and lineage commitment in <em>Plasmodium falciparum</em> reveals a homogeneous pattern of transcription in ex vivo culture.</strong> Jacob Lemieux*, Natalia Gomez-Escobar*, Avi Feller*, Celine Carret, Alfred Amambua-Ngwa, Robert Pinches, Felix Daya, Sue Kyes, David Conway, Chris Holmes, and Chris Newbold (* indicates equal contribution). <em>Proceedings of the National Academy of Sciences.</em> 2009. 106(18): 7559–7564. [<a href="#" data-bs-target="#pnas_1_abstract" data-bs-toggle="collapse" class="text-decoration-none">abstract</a>; <a href="http://www.pnas.org/content/106/18/7559.short" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<div id="pnas_1_abstract" class="collapse"><div class="card card-body">We have cultured Plasmodium falciparum directly from the blood of infected individuals to examine patterns of mature-stage gene expression in patient isolates. Analysis of the transcriptome of P. falciparum is complicated by the highly periodic nature of gene expression because small variations in the stage of parasite development between samples can lead to an apparent difference in gene expression values. To address this issue, we have developed statistical likelihood-based methods to estimate cell cycle progression and commitment to asexual or sexual development lineages in our samples based on microscopy and gene expression patterns. In cases subsequently matched for temporal development, we find that transcriptional patterns in ex vivo culture display little variation across patients with diverse clinical profiles and closely resemble transcriptional profiles that occur in vitro. These statistical methods, available to the research community, assist in the design and interpretation of P. falciparum expression profiling experiments where it is difficult to separate true differential expression from cell-cycle dependent expression. We reanalyze an existing dataset of in vivo patient expression profiles and conclude that previously observed discrete variation is consistent with the commitment of a varying proportion of the parasite population to the sexual development lineage.</div><br></div>
<h2 id="comments" class="anchored">Comments</h2>
<p><strong>Comment on ‘Causal Inference Using Invariant Prediction: Identification and Confidence Intervals’ by Peters, Buehlmann, and Meinshausen.</strong> Peng Ding and Avi Feller. <em>Journal of the Royal Statistical Society (Series B)</em> 2016. 78(5): 994-995. [<a href="http://onlinelibrary.wiley.com/doi/10.1111/rssb.12167/full" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>
<p><strong>Comment on ‘How to find an appropriate clustering for mixed type variables with application to socio-economic stratification’ by Hennig and Liao.</strong> Avi Feller and Edo Airoldi. <em>Journal of the Royal Statistical Society (Series C)</em> 2013. 62(3): 347–348. [<a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9876.2012.01066.x/abstract" class="text-decoration-none" target="_blank"><i class="ai ai-archive" role="img" aria-label="published"></i> published</a>]</p>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>